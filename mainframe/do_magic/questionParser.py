import nltk
import do_magic.voila as voila
import do_magic.dataQuery as sqlQuery
import do_magic.answerFinder as answer
from nltk.corpus import wordnet

tableList = ["phrase", "player_data", "stats"]

# return from this statement " ("select * from phrase join lookup_table as LU on phrase.FK=LU.PK where Phrase like "
#              + "'%" + word + "%'") "
# is : [('tall', 56, 56, 'height', 5, 'player_data')]
# 0 = reference 1 = FK 2 = PK 3 = name of column 4 = column number (for getting answer). 5 is table where lookup is found
# also for building SQL statement when getting answer.

# todo I finished general triangulation used wordnet to check stop words after spell check after lookup table.  There is
#   a new DB dump -- fix the insert statement first using weight before trying to upload the new DB tables. This will
#    a wider range of items you can test the insert statment on to verify it is correctly inserting. then we can use
#     just a bunch of questions to "train" this model and insert the words by asking questions.  this way we use as little
#      human interaction as possible (meaning less calls to candidates) so its a more "automated" system.
# https://www.geeksforgeeks.org/get-synonymsantonyms-nltk-wordnet-python/

# todo capture dates/year ex: how many points did larry bird have in 1971?

#words == non proper nouns.

#general algo flow:
# 1. separate nouns and words. Proper nouns into Nouns, all regular nouns in words and nouns. else words.
# 2. run query for WORDS to find the table and column to search
# 2a. If no result is found for WORDS take nonmatched words pass into Wordnet finder.


def parseQuestion(question):
    #todo start
    tokenized = nltk.word_tokenize(question)
    tagged_sentence = voila.tag_Sentence(tokenized)
    tagged_sentence = voila.apostrophefix(tagged_sentence)
    nouns = [] #list of name words
    words = [] #list of non name words
    nonMatchedWord = [] #list of words (NOT NOUNS) which may be used to break ties with spell checker.
    nameResults = []  #list of results generated by noun search.
    wordResults = [] #list of results generated by nonNouns
    nonMatchedNouns = []

    category = "unknown" #could be used for flag info or not whatever


    #Separates nouns from nonNouns.  Parses table for match.
    for word in tagged_sentence:
        if word[1] == "NNP":
            nouns.append(word[0])
            category = "Person"
        elif word[1] == "NN":
            nouns.append(word[0])
            words.append(word[0])
        else:
            words.append(word[0])

    #searches for word in look up table.
    wordList = voila.get_stopwords(words)

    for word in wordList:
        phrase_result = sqlQuery.dbQuery("select * from phrase join lookup_table as LU on phrase.FK=LU.PK where Phrase"
                                         " like " + "'%" + word + "%' COLLATE utf8_general_ci")
        if phrase_result:
            wordResults.append(phrase_result)
        else:
            nonMatchedWord.append(word)

    #begins triangulation of all words (non proper nouns) from lookup table.
    wordResults = answer.triangulate(wordResults)

    #if no results check for wordnet hits and triangulates.
    if len(wordResults) == 0:
        wordResults = answer.triangulate(wordNetResults(nonMatchedWord))

    #ensures correct form of the
    if len(wordResults) > 1:
        results = answer.triangulate(breakTie(nonMatchedWord))
        if len(results) > 0:
            wordResults.append(results)
        wordResults = answer.triangulate(wordResults)


    if len(wordResults) > 0:
        while isinstance(wordResults[0], tuple):
            wordResults = answer.flatten(wordResults)

    #todo insert check here to validate wordresults isn't empty
    # if this is empty you can run candidates, then start looking at "bad returns".


    #sets basic SQL begining
    selectStatment = "select * from player_data where name like "

    for table in tableList:
        if table == wordResults[5]:
            selectStatment = "select * from " + table + " where name like "


    # searches for direct name look up from table returned from non-noun check.
    for noun in nouns:
        result = sqlQuery.dbQuery(selectStatment + "'%" + noun + "%' COLLATE utf8_general_ci")
        if result:
            nameResults.append(result)
        else:
            nonMatchedWord.append(noun)

    nameResults = answer.triangulate(nameResults)

    if len(nameResults) > 1:
        results = answer.triangulate(breakTie(nonMatchedNouns))
        if len(results) > 0:
            nameResults.append(results)
        nameResults = answer.triangulate(nameResults)

    #this is ONLY FOR 1 to 1 look ups
    # todo we might need to create an enum for flags  Could possibly use lemma searching for NOUNS.




    if len(nameResults) > 0:
        while isinstance(nameResults[0], tuple):
            nameResults = answer.flatten(nameResults)

    #todo this is the next use of candidates! word net will not return syns for name... duh.

    #todo end

    #todo this is the start of the switch statements I believe -- each case would require a different
    # use of the results.

    playerName = voila.singlequoteSQLfix(nameResults[0])
    tableName = voila.singlequoteSQLfix(wordResults[5])
    finalAnswer = answer.flatten(sqlQuery.dbQuery("select * from "+ tableName + " where name =" + "'" + playerName + "'"))

    finalIndex = wordResults[4]

    return finalAnswer[finalIndex]


def breakTie(nonMatched):
    nonMatched = voila.get_stopwords(nonMatched)
    searchMatch = []
    for word in nonMatched:
        # if it already has double quotes that means its ready to be put into sql query and will not go through spellcheck
        if word.find("''") != -1:
            refined_word = word
        # if it doesnt have double quotes run the spell check
        else:
            refined_word = voila.spell_check(word)
            # if spell check returns something back like o'neal. o'neal is NOT sql query safe. so we need to make it o''
            # neal to make it sql query safe
            if refined_word:
                refined_word = voila.singlequoteSQLfix(refined_word)
                for table in tableList:
                    result = sqlQuery.dbQuery("select * from "+ table + " where * like "+ "'%" + refined_word + "%'")
                    if result:
                        searchMatch.append(result)
    return searchMatch


#todo test the insert statement.  The idea is that if a lemma gets a match we add the word which the lemma
# was derived from to the phrase table with the PK found by the lemma.  This will reduce the number of calls to this
#  function so that we don't have this terrible o(n^3) function called more than it needs to be.
def wordNetResults(nonMatched):
    results = []
    for word in nonMatched:
        for syn in wordnet.synsets(word):
            for lemmas in syn.lemmas():
                print(lemmas.name())
                result = sqlQuery.dbQuery("select * from phrase join lookup_table as LU on phrase.FK=LU.PK where Phrase"
                                          " like " + "'%" + lemmas.name() + "%'")
                if result:
                    results.append(result)
                    print(f"INSERT INTO phrase (Phrase, FK) VALUES ({word}, {result[1]})")
                    sqlQuery.dbInsert(f"INSERT INTO phrase (Phrase, FK) VALUES ({word}, {result[1]})")

    return results





