import nltk
import do_magic.voila as voila
import do_magic.dataQuery as sqlQuery
import do_magic.answerFinder as answer
from nltk.corpus import wordnet

tableList = ["phrase", "player_data", "stats"]


# return from this statement " ("select * from phrase join lookup_table as LU on phrase.FK=LU.PK where Phrase like "
#              + "'%" + word + "%'") "
# is : [('tall', 56, 56, 'height', 5, 'player_data')]
# 0 = reference 1 = FK 2 = PK 3 = name of column 4 = column number (for getting answer). 5 is table where lookup is found
# also for building SQL statement when getting answer.

# todo I finished general triangulation used wordnet to check stop words after spell check after lookup table.  There is
#   a new DB dump -- fix the insert statement first using weight before trying to upload the new DB tables. This will
#    a wider range of items you can test the insert statment on to verify it is correctly inserting. then we can use
#     just a bunch of questions to "train" this model and insert the words by asking questions.  this way we use as little
#      human interaction as possible (meaning less calls to candidates) so its a more "automated" system.
# https://www.geeksforgeeks.org/get-synonymsantonyms-nltk-wordnet-python/

# todo capture dates/year ex: how many points did larry bird have in 1971?

# words == non proper nouns.

# general algo flow:
# 1. separate nouns and words. Proper nouns into Nouns, all regular nouns in words and nouns. else words.
# 2. run query for WORDS to find the table and column to search
# 2a. If no result is found for WORDS take nonmatched words pass into Wordnet finder.


def parseQuestion(question):
    # todo start
    tokenized = nltk.word_tokenize(question)
    # tagged_sentence = voila.tag_Sentence(tokenized)
    # tagged_sentence = voila.apostrophefix(tagged_sentence)
    nouns = []  # list of name words
    words = []  # list of non name words
    nonMatchedWord = []  # list of words (NOT NOUNS) which may be used to break ties with spell checker.
    playerResults = []  # list of results generated by noun search.
    wordResults = []  # list of results generated by nonNouns
    nonMatchedNouns = []
    statsResults = []

    category = "unknown"  # could be used for flag info or not whatever

    tokenized = voila.get_stopwords(tokenized)
    for word in tokenized:

        stats = search_stats_DB(word)
        player = search_player_dB(word)
        if player:
            playerResults.append(player)
        if stats:
            statsResults.append(stats)
        else:
            phrase = search_phrase_DB(word)
            if phrase:
                wordResults.append(phrase)
            else:
                nonMatchedWord.append(word)

    # makes it 1 array
    if wordResults:
        wordResults = clean_up_N_arrays(wordResults)



    # Separates nouns from nonNouns.  Parses table for match.
    # for word in tagged_sentence:
    #     if word[1] == "NNP":
    #         nouns.append(word[0])
    #         category = "Person"
    #     elif word[1] == "NN":
    #         nouns.append(word[0])
    #         words.append(word[0])
    #     else:
    #         words.append(word[0])

    # searches for word in look up table.
    # wordList = voila.get_stopwords(words)

    # for word in wordList:
    #     phrase_result = sqlQuery.dbQuery("select * from phrase join lookup_table as LU on phrase.FK=LU.PK where Phrase"
    #                                      " like " + "'%" + word + "%' COLLATE utf8_general_ci")
    #     if phrase_result:
    #         wordResults.append(phrase_result)
    #     else:
    #         nonMatchedWord.append(word)

    # begins triangulation of all words (non proper nouns) from lookup table.
    # wordResults = answer.triangulate(wordResults)

    # if no results check for wordnet hits and triangulates.
    # if len(wordResults) == 0:
    #     wordResults = answer.triangulate(wordNetResults(nonMatchedWord))
    #
    # # ensures correct form of the
    # if len(wordResults) > 1:
    #     results = answer.triangulate(breakTie(nonMatchedWord))
    #     if len(results) > 0:
    #         wordResults.append(results)
    #     wordResults = answer.triangulate(wordResults)
    #
    # if len(wordResults) > 0:
    #     while isinstance(wordResults[0], tuple):
    #         wordResults = answer.flatten(wordResults)

    # todo insert check here to validate wordresults isn't empty
    # if this is empty you can run candidates, then start looking at "bad returns".

    # sets basic SQL begining
    # selectStatment = "select * from player_data where name like "
    #
    # for table in tableList:
    #     if table == wordResults[5]:
    #         selectStatment = "select * from " + table + " where LOWER(name) LIKE LOWER"
    #
    # # searches for direct name look up from table returned from non-noun check.
    # for noun in nouns:
    #     result = sqlQuery.dbQuery(selectStatment + "('%" + noun + "%') COLLATE utf8_general_ci")
    #     if result:
    #         playerResults.append(result)
    #     else:
    #         nonMatchedWord.append(noun)
    #
    playerResults = processResults(playerResults, nonMatchedWord)

    # # this is ONLY FOR 1 to 1 look ups
    # # todo we might need to create an enum for flags  Could possibly use lemma searching for NOUNS.
    #
    # todo this is the next use of candidates! word net will not return syns for name... duh.

    # todo end

    # todo this is the start of the switch statements I believe -- each case would require a different
    # use of the results.

    if len(playerResults) > 0:
        playerName = voila.singlequoteSQLfix(playerResults[0])
    elif len(statsResults) > 0:
        print(statsResults[0])
        playerName = voila.singlequoteSQLfix(statsResults[0])

    tableName = voila.singlequoteSQLfix(wordResults[5])
    finalAnswer = answer.flatten(
        sqlQuery.dbQuery("select * from " + tableName + " where name =" + "'" + playerName + "'"))

    finalIndex = wordResults[4]

    return finalAnswer[finalIndex]


def breakTie(nonMatched):
    nonMatched = voila.get_stopwords(nonMatched)
    searchMatch = []
    for word in nonMatched:
        # if it already has double quotes that means its ready to be put into sql query and will not go through spellcheck
        if word.find("''") != -1:
            refined_word = word
        # if it doesnt have double quotes run the spell check
        else:
            refined_word = voila.spell_check(word)
            # if spell check returns something back like o'neal. o'neal is NOT sql query safe. so we need to make it o''
            # neal to make it sql query safe
            if refined_word:
                refined_word = voila.singlequoteSQLfix(refined_word)
                for table in tableList:
                    result = sqlQuery.dbQuery("select * from " + table + " where * like " + "'%" + refined_word + "%'")
                    if result:
                        searchMatch.append(result)
    return searchMatch


# todo test the insert statement.  The idea is that if a lemma gets a match we add the word which the lemma
# was derived from to the phrase table with the PK found by the lemma.  This will reduce the number of calls to this
#  function so that we don't have this terrible o(n^3) function called more than it needs to be.
def wordNetResults(nonMatched):
    results = []
    for word in nonMatched:
        for syn in wordnet.synsets(word):
            for lemmas in syn.lemmas():
                print(lemmas.name())
                result = sqlQuery.dbQuery("select * from phrase join lookup_table as LU on phrase.FK=LU.PK where Phrase"
                                          " like " + "'%" + lemmas.name() + "%'")
                if result:
                    results.append(result)
                    print(f"INSERT INTO phrase (Phrase, FK) VALUES ({word}, {result[1]})")
                    sqlQuery.dbInsert(f"INSERT INTO phrase (Phrase, FK) VALUES ({word}, {result[1]})")

    return results


def search_player_dB(word):
    result = sqlQuery.dbQuery(
        "select * from player_data where LOWER(name) LIKE LOWER ('%" + word + "%') COLLATE utf8_general_ci")
    if result:
        return result


def search_stats_DB(word):
    result = sqlQuery.dbQuery(
        "select * from stats where LOWER(Player) LIKE LOWER ('%" + word + "%') COLLATE utf8_general_ci")
    if result:
        return result


def search_phrase_DB(word):
    result = sqlQuery.dbQuery("select * from phrase join lookup_table as LU on phrase.FK=LU.PK where Phrase"
                              " like " + "'%" + word + "%' COLLATE utf8_general_ci")
    if result:
        return result


def clean_up_N_arrays(array):
    # if
    while isinstance(array[0], tuple) or isinstance(array[0], list):
        array = answer.flatten(array)
    return array


def processResults(playerResults, nonMatchedNouns):
    playerResults = answer.triangulate(playerResults)

    #
    if len(playerResults) > 1:
        results = answer.triangulate(breakTie(nonMatchedNouns))
        if len(results) > 0:
            playerResults.append(results)
        playerResults = answer.triangulate(playerResults)

    if len(playerResults) > 0:
        while isinstance(playerResults[0], tuple):
            playerResults = answer.flatten(playerResults)

    return playerResults
    #
